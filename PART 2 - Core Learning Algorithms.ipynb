{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUgsCvCHLksw"
   },
   "source": [
    "# TensorFlow Core Learning Algorithms\n",
    "In this notebook we will walk through 4 fundemental machine learning algorithms. We will apply each of these algorithms to unique problems and datasets before highlighting the use cases of each.\n",
    "\n",
    "The algorithms we will focus on include:\n",
    "- Linear Regression\n",
    "- Classification\n",
    "- Clustering\n",
    "- Hidden Markov Models\n",
    "\n",
    "It is worth noting that there are many tools within TensorFlow that could be used to solve the problems we will see below. I have chosen the tools that I believe give the most variety and are easiest to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbdnKlcuMM4u"
   },
   "source": [
    "## Linear Regression\n",
    "Linear regression is one of the most basic forms of machine learning and is used to predict numeric values.\n",
    "\n",
    "In this tutorial we will use a linear model to predict the survival rate of passangers from the titanic dataset.\n",
    "\n",
    "*This section is based on the following documentation: https://www.tensorflow.org/tutorials/estimator/linear*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsJ-Iov1P4RK"
   },
   "source": [
    "### How it Works\n",
    "Before we dive in, I will provide a very surface level explanation of the linear regression algorithm.\n",
    "\n",
    "**Linear regression :** follows a very simple concept, if data points are related linearly, we can generate a line of best fit for these points and use it to predict future values.\n",
    "\n",
    "Let's take an example of a data set with one feature and one label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "t4Hp-dYmR2jf",
    "outputId": "db401a27-85b0-4ec2-a110-91c938f54bca"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = [1, 2, 2.5, 3, 4]\n",
    "y = [1, 4, 7, 9, 15]\n",
    "plt.plot(x, y, 'ro')\n",
    "plt.axis([0, 6, 0, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2m1DTp22SLo3"
   },
   "source": [
    "We can see that this data has a linear coorespondence. When the x value increases, so does the y. Because of this relation we can create a line of best fit for this dataset. In this example our line will only use one input variable, as we are working with two dimensions. In larger datasets with more features our line will have more features and inputs.\n",
    "\n",
    "\"Line of best fit refers to a line through a scatter plot of data points that best expresses the relationship between those points.\" (https://www.investopedia.com/terms/l/line-of-best-fit.asp)\n",
    "\n",
    "Here's a refresher on the equation of a line in 2D.\n",
    "\n",
    "$ y = mx + b $\n",
    "\n",
    "Here's an example of a line of best fit for this graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "Kv5eKLP_UYZi",
    "outputId": "77c4f7fd-d343-4f60-a7a0-ebeecd253ef9"
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'ro')\n",
    "plt.axis([0, 6, 0, 20])\n",
    "plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hd3T50PLB4lV"
   },
   "source": [
    "Once we've generated this line for our dataset, we can use its equation to predict future values. We just pass the features of the data point we would like to predict into the equation of the line and use the output as our prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y02FbC56Nbx0"
   },
   "source": [
    "### Setup and Imports\n",
    "Before we get started we must install *sklearn* and import the following modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4xpuxwKHOGL"
   },
   "outputs": [],
   "source": [
    "!pip install -q sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "id": "iI3zi2ZhQ3WB",
    "outputId": "9b7fa093-1a82-481a-fabe-1cb44429919d"
   },
   "outputs": [],
   "source": [
    "#%tensorflow_version 2.x  # this line is not required unless you are in a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qcII_xj9Ntyo"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from six.moves import urllib\n",
    "\n",
    "import tensorflow.compat.v2.feature_column as fc\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GltdTjiERfWi"
   },
   "source": [
    "### Data\n",
    "So, if you haven't realized by now, a major part of machine learning is data! In fact, it's so important that most of what we do in this tutorial will focus on exploring, cleaning and selecting appropriate data.\n",
    "\n",
    "The dataset we will be focusing on here is the titanic dataset. It has tons of information about each passanger on the ship. Our first step is always to understand the data and explore it. So, let's do that!\n",
    "\n",
    "**Below we will load a dataset and learn how we can explore it using some built-in tools.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpllWsKIOGOy"
   },
   "outputs": [],
   "source": [
    "# Load dataset.\n",
    "dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') # training data\n",
    "dfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # testing data\n",
    "y_train = dftrain.pop('survived')\n",
    "y_eval = dfeval.pop('survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PJr5GosQBeY"
   },
   "source": [
    "The ```pd.read_csv()``` method will return to us a new pandas *dataframe*. One might think of a dataframe like a table. In fact, we can actually have a look at the table representation.\n",
    "\n",
    "We've decided to pop the \"survived\" column from our dataset and store it in a new variable (the target). This column simply tells us if the person survived or not.\n",
    "\n",
    "To look at the data we'll use the ```.head()``` method from pandas. This will show us the first 5 items in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "mKsXeWinQiVR",
    "outputId": "b91f8b23-eb7b-4fa6-ccd7-aef6d9441afa"
   },
   "outputs": [],
   "source": [
    "dftrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eeVQGdpCRC1P"
   },
   "source": [
    "And if we want a more statistical analysis of our data we can use the ```.describe()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "IkL2G42GRMf8",
    "outputId": "ffb42606-8f18-48d2-aacf-120578212af7"
   },
   "outputs": [],
   "source": [
    "dftrain.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aX7FSzrQRXeC"
   },
   "source": [
    "And since we talked so much about shapes in the previous tutorial let's have a look at that too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "tR1Oy1dISdjn",
    "outputId": "94fb254b-0222-4b4b-d47c-73562f37571c"
   },
   "outputs": [],
   "source": [
    "dftrain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIfLiSRMTeW4"
   },
   "source": [
    "So have have 627 entries and 9 features, nice!\n",
    "\n",
    "Now let's have a look at our survival information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "id": "aX1lKW7TTh-E",
    "outputId": "6b2efa46-40a8-4fae-bdd1-b828283daa0c"
   },
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahqIyYPoTnRh"
   },
   "source": [
    "Notice that each entry is either a 0 or 1. Can you guess which stands for survival?...the answer is 1.\n",
    "\n",
    "**And now because visuals are always valuable let's generate a few graphs of the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "Edndbw4sU5Wd",
    "outputId": "42a09890-5513-4785-84d7-98af53216446"
   },
   "outputs": [],
   "source": [
    "dftrain.age.hist(bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "4SM_tYvyUtsw",
    "outputId": "128bea12-3654-4692-e7cc-6279f8ed2079"
   },
   "outputs": [],
   "source": [
    "dftrain.sex.value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "WCv3ek2LU1Lw",
    "outputId": "a71bfdd1-ebbf-44cf-ad1b-9ec992e2fcd7"
   },
   "outputs": [],
   "source": [
    "dftrain['class'].value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "D4kPWqBYVDlj",
    "outputId": "c4f858de-73e4-41a3-8928-e97a0e0e08ad"
   },
   "outputs": [],
   "source": [
    "pd.concat([dftrain, y_train], axis=1).groupby('sex').survived.mean().plot(kind='barh').set_xlabel('% survive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l42qmk_bVHvD"
   },
   "source": [
    "After analyzing these informations, we should notice the following:\n",
    "- Most passengers are in their 20's or 30's\n",
    "- Most passengers are male\n",
    "- Most passengers are in \"Third\" class\n",
    "- Females have a much higher chance of survival\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIBZww6kOIAp"
   },
   "source": [
    "### Training vs Testing Data\n",
    "One may have noticed that we loaded **two different datasets** above. This is because when we train models, we need two sets of data: **training and testing**.\n",
    "\n",
    "The **training** data is what we feed to the model so that it can develop and learn. It is usually a much larger size than the testing data.\n",
    "\n",
    "The **testing** data is what we use to evaluate the model and see how well it is performing. We must use a separate set of data that the model has not been trained on to evaluate it. Can you think of why this is?\n",
    "\n",
    "Well, the point of our model is to be able to make predictions on NEW data, data that we have never seen before. If we simply test the model on the data that it has already seen we cannot measure its accuracy accuratly. We can't be sure that the model hasn't simply memorized our training data. This is why we need our testing and training data to be seperate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ar_cXv2jV8A3"
   },
   "source": [
    "### Feature Columns\n",
    "In our dataset we have two different kinds of information: **Categorical and Numeric**\n",
    "\n",
    "Our **categorical data** is anything that is not numeric! For example, the `sex` column does not use numbers, it uses the words \"male\" and \"female\".\n",
    "\n",
    "Before we continue and create/train a model, we must convert our categorical data into numeric data. We can do this by encoding each category with an integer (ex. male = 1, female = 2).\n",
    "\n",
    "Fortunately for us TensorFlow has some tools to help!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tf.feature_column.categorical_column_with_vocabulary_list` function is used in TensorFlow to define a categorical feature column with a predefined vocabulary list. This function is commonly used when working with categorical features in machine learning models, such as those used in deep learning models.\n",
    "\n",
    "Here's what each parameter of categorical_column_with_vocabulary_list does:\n",
    "\n",
    "    * `feature_name` : This parameter specifies the name of the feature column. It is a string that identifies the feature in the dataset.\n",
    "\n",
    "    * `vocabulary` : This parameter is a list of strings representing the vocabulary of the categorical feature. It contains all possible values that the feature can take.\n",
    "\n",
    "When you create a feature column using categorical_column_with_vocabulary_list, TensorFlow will map each value in the vocabulary list to a unique integer ID. During training or inference, TensorFlow will convert the categorical feature values to these integer IDs, which can then be used as input to a machine learning model.\n",
    "\n",
    "The `tf.feature_column.numeric_column` function in TensorFlow is used to create a numeric feature column. Numeric feature columns are used to represent continuous-valued features in a dataset, such as age, income, temperature, etc. These columns are typically used as input to machine learning models, including deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "-lcnwG0VXF5h",
    "outputId": "a4603a14-5903-4e75-a8cf-6a2ccca69946"
   },
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',\n",
    "                       'embark_town', 'alone']\n",
    "NUMERIC_COLUMNS = ['age', 'fare']\n",
    "\n",
    "feature_columns = []\n",
    "for feature_name in CATEGORICAL_COLUMNS :\n",
    "    vocabulary = dftrain[feature_name].unique()  # gets a list of all unique values from given feature column\n",
    "    feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))\n",
    "\n",
    "for feature_name in NUMERIC_COLUMNS :\n",
    "    feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))\n",
    "\n",
    "print(feature_columns)\n",
    "print(type(feature_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-nazdZpXgAr"
   },
   "source": [
    "Let's break this code down a little bit...\n",
    "\n",
    "Essentially what we are doing here is creating a list of features that are used in our dataset.\n",
    "\n",
    "The cryptic lines of code inside the ```append()``` create an object that our model can use to map string values like \"male\" and \"female\" to integers. This allows us to avoid manually having to encode our dataframes.\n",
    "\n",
    "*And here is some relevant documentation*\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list?version=stable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQlXWErlbhsG"
   },
   "source": [
    "### The Training Process\n",
    "So, we are almost done preparing our dataset and I feel as though it's a good time to explain how our model is trained. Specifically, how input data is fed to our model.\n",
    "\n",
    "For this specific model data is going to be streamed into it in **small batches of 32**. This means we will not feed the entire dataset to our model at once, but simply small batches of entries. We will feed these batches to our model multiple times according to the number of **epochs**.\n",
    "\n",
    "An **epoch** is simply one stream of our entire dataset. The number of epochs we define is the amount of times our model will see the entire dataset. We use multiple epochs in hope that after seeing the same data multiple times the model will better determine how to estimate it.\n",
    "\n",
    "* Example : if we have 10 epochs, our model will see the same dataset 10 times.\n",
    "\n",
    "Since we need to feed our data in batches and multiple times, we need to create something called an **input function**. \n",
    "The **input function** simply defines how our dataset will be converted into batches at each epoch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OO0mBu_WaVXp"
   },
   "source": [
    "### Input Function\n",
    "The TensorFlow model we are going to use requires that the data we pass in comes in as a ```tf.data.Dataset``` object. This means we must create a *input function* that can convert our current pandas dataframe into that object.\n",
    "\n",
    "Below you'll see a seemingly complicated input function, this is straight from the TensorFlow documentation (https://www.tensorflow.org/tutorials/estimator/linear). I've commented as much as I can to make it understandble, but you may want to refer to the documentation for a detailed explanation of each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3qcbvOYbIwa"
   },
   "outputs": [],
   "source": [
    "def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):\n",
    "    def input_function():  # inner function, this will be returned\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))  # create tf.data.Dataset object with data and its label\n",
    "        if shuffle :\n",
    "            ds = ds.shuffle(1000)  # randomize order of data\n",
    "        ds = ds.batch(batch_size).repeat(num_epochs)  # split dataset into batches of 32 and repeat process for number of epochs\n",
    "        return ds  # return a batch of the dataset\n",
    "    return input_function  # return a function object for use\n",
    "\n",
    "train_input_fn = make_input_fn(dftrain, y_train)  # here we will call the input_function that was returned to us to get a dataset object we can feed to the model\n",
    "eval_input_fn = make_input_fn(dfeval, y_eval, num_epochs=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXqlPst9fpx4"
   },
   "source": [
    "### Creating the Model\n",
    "In this notebook we are going to use a linear estimator to utilize the linear regression algorithm.\n",
    "\n",
    "Creating one is pretty easy! Have a look below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192
    },
    "id": "q1Wo8brFf663",
    "outputId": "73b78e55-1fc4-44b0-9fc6-33506a199db5"
   },
   "outputs": [],
   "source": [
    "# We create a linear estimtor by passing the feature columns we created earlier\n",
    "linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5GPkW_CgDFy"
   },
   "source": [
    "### Training the Model\n",
    "Training the model is as easy as passing the input functions that we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "J11OJrlZgPhb",
    "outputId": "cd3c7099-fdde-45aa-b57b-f1c8d1d02024"
   },
   "outputs": [],
   "source": [
    "linear_est.train(train_input_fn)  # train\n",
    "result = linear_est.evaluate(eval_input_fn)  # get model metrics/stats by testing on tetsing data\n",
    "\n",
    "clear_output()  # clears consolle output\n",
    "print(result['accuracy'])  # the result variable is simply a dict of stats about our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LisxO81tgi1n"
   },
   "source": [
    "And we now we have a model with a 74% accuracy (this will change each time)! Not crazy impressive but decent for our first try.\n",
    "\n",
    "Now let's see how we can actually use this model to make predicitons.\n",
    "\n",
    "We can use the ```.predict()``` method to get survival probabilities from the model. This method will return a list of dicts that store a predicition for each of the entries in our testing data set. Below we've used some pandas magic to plot a nice graph of the predictions.\n",
    "\n",
    "As you can see the survival rate is not very high :/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525
    },
    "id": "JQz0Lj60hjLI",
    "outputId": "eec262fd-e206-46b6-d8d7-7c9067546f16"
   },
   "outputs": [],
   "source": [
    "pred_dicts = list(linear_est.predict(eval_input_fn))\n",
    "probs = pd.Series([pred['probabilities'][1] for pred in pred_dicts])\n",
    "\n",
    "probs.plot(kind='hist', bins=20, title='predicted probabilities');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CN_r0Vn8VOf5"
   },
   "source": [
    "That's it for linear regression! Now onto classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hG9gxhAqVTBT"
   },
   "source": [
    "## Classification\n",
    "Now that we've covered linear regression it is time to talk about classification. Where regression was used to predict a numeric value, classification is used to seperate data points into classes of different labels. In this example we will use a TensorFlow estimator to classify flowers.\n",
    "\n",
    "Since we've touched on how estimators work earlier, I'll go a bit quicker through this example.\n",
    "\n",
    "This section is based on the following guide from the TensorFlow website.\n",
    "https://www.tensorflow.org/tutorials/estimator/premade\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWk2Kb7Sdk-T"
   },
   "source": [
    "### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "id": "eH4_xJaD605_",
    "outputId": "cbe50069-f9c1-489e-ebdb-e08af4d7cea5"
   },
   "outputs": [],
   "source": [
    "#%tensorflow_version 2.x  # this line is not required unless you are in a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMiLc6LPdoPm"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zLNzmkGds1U"
   },
   "source": [
    "### Dataset\n",
    "This specific dataset seperates flowers into 3 different classes of species.\n",
    "- Setosa\n",
    "- Versicolor\n",
    "- Virginica\n",
    "\n",
    "The information about each flower is the following.\n",
    "- sepal length\n",
    "- sepal width\n",
    "- petal length\n",
    "- petal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "puOQDTNKeCRC"
   },
   "outputs": [],
   "source": [
    "# Lets define some constants to help us later on\n",
    "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n",
    "SPECIES = ['Setosa', 'Versicolor', 'Virginica']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "oMW41Wd9eLIo",
    "outputId": "4950908e-de50-4839-a569-a1f5acf015f8"
   },
   "outputs": [],
   "source": [
    "# Here we use keras (a module inside of TensorFlow) to grab our datasets and read them into a pandas dataframe\n",
    "train_path = tf.keras.utils.get_file(\n",
    "    \"iris_training.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\")\n",
    "test_path = tf.keras.utils.get_file(\n",
    "    \"iris_test.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\")\n",
    "\n",
    "train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aHRWY47ecdr"
   },
   "source": [
    "Let's have a look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "BQ9uo6KkegBH",
    "outputId": "ed4b9d06-06da-4014-f04d-95bc35dc64ba"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PzWyoE9eu8H"
   },
   "source": [
    "Now we can pop the species column off and use that as our label (i.e. target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "fP_nlslke4U8",
    "outputId": "560a0559-ebf8-43a8-bca7-4fa25d9c06ce"
   },
   "outputs": [],
   "source": [
    "train_y = train.pop('Species')\n",
    "test_y = test.pop('Species')\n",
    "train.head() # the species column is now gone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "3oVw2zRkfTXq",
    "outputId": "a25c814d-63e4-40da-9ac9-6efc326164b2"
   },
   "outputs": [],
   "source": [
    "train.shape  # we have 120 entires with 4 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6ZWVZs8fg-H"
   },
   "source": [
    "### Input Function\n",
    "Remember that nasty input function we created earlier. Well, we need to make another one here! Fortunatly for us this one is a little easier to digest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K-NQRLv_fyhg"
   },
   "outputs": [],
   "source": [
    "def input_fn(features, labels, training=True, batch_size=256):\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Shuffle and repeat if you are in training mode.\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(1000).repeat()\n",
    "\n",
    "    return dataset.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FL--3OAnf4mO"
   },
   "source": [
    "### Feature Columns\n",
    "And you didn't think we forgot about the feature columns, did you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "nErIJJbggQ5w",
    "outputId": "837852b1-97a0-4965-88fc-50883dfb558c"
   },
   "outputs": [],
   "source": [
    "# Feature columns describe how to use the input.\n",
    "my_feature_columns = []\n",
    "for key in train.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
    "print(my_feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kl1Wr_Xgpmv"
   },
   "source": [
    "### Building the Model\n",
    "And now we are ready to choose a model. For classification tasks there are variety of different estimators/models that we can pick from. Some options are listed below.\n",
    "- ```DNNClassifier``` (Deep Neural Network)\n",
    "- ```LinearClassifier```\n",
    "\n",
    "We can choose either model but the DNN seems to be the best choice. This is because we may not be able to find a linear coorespondence in our data.\n",
    "\n",
    "So let's build a model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192
    },
    "id": "n7YVQowgiDak",
    "outputId": "bd9457f3-7d30-456c-aa8b-3d160c0f751f"
   },
   "outputs": [],
   "source": [
    "# Build a DNN with 2 hidden layers with 30 and 10 hidden nodes each.\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 30 and 10 nodes respectively.\n",
    "    hidden_units=[30, 10],\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQ_SJAMuiF6p"
   },
   "source": [
    "What we've just done is created a deep neural network that has two hidden layers. These layers have 30 and 10 neurons respectively. This is the number of neurons the TensorFlow official tutorial uses so we'll stick with it. However, it is worth mentioning that the number of hidden neurons is an arbitrary number and many experiments and tests are usually done to determine the best choice for these values. We can try playing around with the number of hidden neurons and see if our results change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBHnWYKTjV5D"
   },
   "source": [
    "### Training\n",
    "Now it's time to train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "INug63pCjaOw",
    "outputId": "3eb6211c-9fd1-4748-8e2e-31e64b4ec698"
   },
   "outputs": [],
   "source": [
    "# We include a lambda to avoid creating an inner function previously\n",
    "\n",
    "classifier.train(\n",
    "    input_fn=lambda: input_fn(train, train_y, training=True),\n",
    "    steps=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57oNBLV1j0wc"
   },
   "source": [
    "The only thing to explain here is the **steps** argument. This simply tells the classifier to run for 5000 steps. We can try modifiying this and seeing if our results change. We  must keep in mind that more is not always better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5suI1lmskE7p"
   },
   "source": [
    "### Evaluation\n",
    "Now let's see how this trained model does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "23rIrgbxkJUO",
    "outputId": "24767659-81cb-436f-a3c5-3fb19fcf53b2"
   },
   "outputs": [],
   "source": [
    "eval_result = classifier.evaluate(\n",
    "    input_fn=lambda: input_fn(test, test_y, training=False))\n",
    "\n",
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4v1ZMe7jkXdp"
   },
   "source": [
    "Notice this time we didn't specify the number of steps. This is because during evaluation the model will only look at the testing data one time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "464HkZ6lknua"
   },
   "source": [
    "### Predictions\n",
    "Now that we have a trained model it's time to use it to make predictions. I've written a little script below that allows you to type the features of a flower and see a prediction for its class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "id": "bQRLq4M1k1jm",
    "outputId": "f720852e-66e9-448e-bddf-ce33ce98cc61"
   },
   "outputs": [],
   "source": [
    "def input_fn(features, batch_size=256):\n",
    "    # Convert the inputs to a Dataset without labels.\n",
    "    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)\n",
    "\n",
    "features = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']\n",
    "predict = {}\n",
    "\n",
    "print(\"Please type numeric values as prompted.\")\n",
    "for feature in features :\n",
    "    valid = True\n",
    "    while valid :\n",
    "        val = input(feature + \": \")\n",
    "        if not val.isdigit(): valid = False # only float nb are accepted\n",
    "\n",
    "    predict[feature] = [float(val)]\n",
    "\n",
    "predictions = classifier.predict(input_fn=lambda: input_fn(predict))\n",
    "for pred_dict in predictions:\n",
    "    class_id = pred_dict['class_ids'][0]\n",
    "    probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "    print('Prediction is \"{}\" ({:.1f}%)'.format(\n",
    "        SPECIES[class_id], 100 * probability))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tRxhpmSr1FH"
   },
   "outputs": [],
   "source": [
    "# Here is some example input and expected classes you can try above\n",
    "expected = ['Setosa', 'Versicolor', 'Virginica']\n",
    "predict_x = {\n",
    "    'SepalLength': [5.1, 5.9, 6.9],\n",
    "    'SepalWidth': [3.3, 3.0, 3.1],\n",
    "    'PetalLength': [1.7, 4.2, 5.4],\n",
    "    'PetalWidth': [0.5, 1.5, 2.1],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujwvc6ASsHID"
   },
   "source": [
    "And that's pretty much it for classification!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0dfaT4esRh3"
   },
   "source": [
    "## Clustering\n",
    "Now that we've covered regression and classification it's time to talk about clustering data!\n",
    "\n",
    "Clustering is a Machine Learning technique that involves the grouping of data points. In theory, data points that are in the same group should have similar properties and/or features, while data points in different groups should have highly dissimilar properties and/or features. (https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)\n",
    "\n",
    "Unfortunalty there are issues with the current version of TensorFlow and the implementation for KMeans. This means we cannot use KMeans without writing the algorithm from scratch. We aren't quite at that level yet, so we'll just explain the basics of clustering for now.\n",
    "\n",
    "#### Basic Algorithm for K-Means.\n",
    "- Step 1: Randomly pick K points to place K centroids\n",
    "- Step 2: Assign all the data points to the centroids by distance. The closest centroid to a point is the one it is assigned to.\n",
    "- Step 3: Average all the points belonging to each centroid to find the middle of those clusters (center of mass). Place the corresponding centroids into that position.\n",
    "- Step 4: Reassign every point once again to the closest centroid.\n",
    "- Step 5: Repeat steps 3-4 until no point changes which centroid it belongs to.\n",
    "\n",
    "*Please refer to the video for an explanation of KMeans clustering.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQ9iJrSbBTZB"
   },
   "source": [
    "## Hidden Markov Models\n",
    "\n",
    "\"The Hidden Markov Model is a finite set of states, each of which is associated with a (generally multidimensional) probability distribution []. Transitions among the states are governed by a set of probabilities called **transition probabilitie**s.\" (http://jedlik.phy.bme.hu/~gerjanos/HMM/node4.html)\n",
    "\n",
    "A hidden markov model works with probabilities to predict future events or states. In this section we will learn how to create a hidden markov model that can predict the weather.\n",
    "\n",
    "*This section is based on the following TensorFlow tutorial.* https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/HiddenMarkovModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKJSFk4NP0eq"
   },
   "source": [
    "### Data\n",
    "Let's start by discussing the type of data we use when we work with a hidden markov model.\n",
    "\n",
    "In the previous sections we worked with large datasets of 100's of different entries. For a markov model we are only interested in probability distributions that have to do with states.\n",
    "\n",
    "We can find these probabilities from large datasets or may already have these values. We'll run through an example in a second that should clear some things up, but let's discuss the components of a markov model.\n",
    "\n",
    "**States:** In each markov model we have a finite set of states. These states could be something like \"warm\" and \"cold\" or \"high\" and \"low\" or even \"red\", \"green\" and \"blue\". These states are \"hidden\" within the model, which means we do not direcly observe them.\n",
    "\n",
    "**Observations:** Each state has a particular outcome or observation associated with it based on a probability distribution. An example of this is the following: *On a hot day Tim has a 80% chance of being happy and a 20% chance of being sad.*\n",
    "\n",
    "**Transitions:** Each state will have a probability defining the likelihood of transitioning to a different state. An example is the following: *a cold day has a 30% chance of being followed by a hot day and a 70% chance of being follwed by another cold day.*\n",
    "\n",
    "To create a hidden markov model we need.\n",
    "\n",
    "    * States\n",
    "    * Observation Distribution\n",
    "    * Transition Distribution\n",
    "\n",
    "For our purpose we will assume we already have this information available as we attempt to predict the weather on a given day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iK2QbOzr6jNJ"
   },
   "source": [
    "### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "id": "Suf1v8kJ6niA",
    "outputId": "f12c7178-cc43-42cd-92c3-8084d5d61637"
   },
   "outputs": [],
   "source": [
    "#%tensorflow_version 2.x  # this line is not required unless you are in a notebook, for google colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GN_Fkrx30xbb"
   },
   "source": [
    "Due to a version mismatch with tensorflow v2 and tensorflow_probability we need to install the most recent version of tensorflow_probability (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "kawrMHKGBWyS",
    "outputId": "ad6d30ff-e53a-4776-825d-c18e2b0728cc"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow_probability==0.8.0rc0 --user --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U dm-sonnet==1.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade tfp-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mEIk7FYD6lcF"
   },
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp  # We are using a different module from tensorflow this time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssOcn-nIOCcV"
   },
   "source": [
    "### Weather Model\n",
    "Taken direclty from the TensorFlow documentation (https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/HiddenMarkovModel).\n",
    "\n",
    "We will model a simple weather system and try to predict the temperature on each day given the following information.\n",
    "1. Cold days are encoded by a 0 and hot days are encoded by a 1.\n",
    "2. The first day in our sequence has an 80% chance of being cold. (initial distribution)\n",
    "3. A cold day has a 30% chance of being followed by a hot day.\n",
    "4. A hot day has a 20% chance of being followed by a cold day.\n",
    "5. On each day the temperature is normally distributed with mean and standard deviation 0 and 5 on a cold day and mean and standard deviation 15 and 10 on a hot day.\n",
    "\n",
    "**standard deviation** can be put simply as the range of expected values.\n",
    "\n",
    "In this example, on a hot day the average temperature is 15 and ranges from 5 to 25.\n",
    "\n",
    "To model this in TensorFlow, we'll do the following :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LBLEJp4YlIf"
   },
   "outputs": [],
   "source": [
    "tfd = tfp.distributions  # making a shortcut for later on\n",
    "initial_distribution = tfd.Categorical(probs=[0.2, 0.8])  # Refer to point 2 above\n",
    "transition_distribution = tfd.Categorical(probs=[[0.7, 0.3],\n",
    "                                                 [0.2, 0.8]])  # refer to points 3 and 4 above ; Stochastic matrix\n",
    "\n",
    "observation_distribution = tfd.Normal(loc=[0., 15.], scale=[5., 10.])  # refer to point 5 above\n",
    "\n",
    "# the loc argument represents the mean and the scale is the standard devitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XtTg0l04mqc"
   },
   "source": [
    "We've now created distribution variables to model our system and it's time to create the hidden markov model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4M6cZww4mZk"
   },
   "outputs": [],
   "source": [
    "model = tfd.HiddenMarkovModel(\n",
    "    initial_distribution=initial_distribution,\n",
    "    transition_distribution=transition_distribution,\n",
    "    observation_distribution=observation_distribution,\n",
    "    num_steps=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJ0XIA2M5gqD"
   },
   "source": [
    "The number of steps represents the number of days that we would like to predict information for. In this case we've chosen 7, an entire week.\n",
    "\n",
    "To get the **expected temperatures** on each day we can do the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "plVVG4fi55Jv",
    "outputId": "cef2d0a3-0edf-4226-ad65-43245933109f"
   },
   "outputs": [],
   "source": [
    "mean = model.mean()\n",
    "\n",
    "# due to the way TensorFlow works on a lower level we need to evaluate part of the graph\n",
    "# from within a session to see the value of this tensor\n",
    "\n",
    "# in the new version of tensorflow we need to use tf.compat.v1.Session() rather than just tf.Session()\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    print(mean.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzzUGR12AkiF"
   },
   "source": [
    "## Conclusion\n",
    "So that's it for the core learning algorithms in TensorFlow. Hopefully we've learned about a few interesting tools that are easy to use!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEeIRxlbx0wY"
   },
   "source": [
    "## Sources\n",
    "\n",
    "1. Chen, James. “Line Of Best Fit.” Investopedia, Investopedia, 29 Jan. 2020, www.investopedia.com/terms/l/line-of-best-fit.asp.\n",
    "2. “Tf.feature_column.categorical_column_with_vocabulary_list.” TensorFlow, www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list?version=stable.\n",
    "3. “Build a Linear Model with Estimators &nbsp;: &nbsp; TensorFlow Core.” TensorFlow, www.tensorflow.org/tutorials/estimator/linear.\n",
    "4. Staff, EasyBib. “The Free Automatic Bibliography Composer.” EasyBib, Chegg, 1 Jan. 2020, www.easybib.com/project/style/mla8?id=1582473656_5e52a1b8c84d52.80301186.\n",
    "5. Seif, George. “The 5 Clustering Algorithms Data Scientists Need to Know.” Medium, Towards Data Science, 14 Sept. 2019, https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68.\n",
    "6. Definition of Hidden Markov Model, http://jedlik.phy.bme.hu/~gerjanos/HMM/node4.html.\n",
    "7. “Tfp.distributions.HiddenMarkovModel &nbsp;: &nbsp; TensorFlow Probability.” TensorFlow, www.tensorflow.org/probability/api_docs/python/tfp/distributions/HiddenMarkovModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "sIBZww6kOIAp",
    "UQlXWErlbhsG"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
